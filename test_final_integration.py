#!/usr/bin/env python3
"""
Teste Final - PitchAI com Whisper Large + AnythingLLM
====================================================
"""

import sys
import json
from pathlib import Path

# Adicionar src ao path
sys.path.insert(0, str(Path(__file__).parent / "src"))

def test_whisper_large_priority():
    """Testa se Whisper Large est√° sendo priorizado"""
    print("üß™ Testando Prioriza√ß√£o Whisper Large...")

    try:
        from src.ai.asr_whisper import TranscriptionService
        from src.ai.model_manager import ModelManager
        from src.core.config import create_config

        config = create_config()
        model_manager = ModelManager(config)

        # Carregar manifesto
        model_manager.load_manifest()
        print(f"‚úÖ Manifesto carregado: {len(model_manager.models)} modelos")

        # Verificar se Whisper Large est√° dispon√≠vel
        large_available = "whisper_large_encoder" in model_manager.models
        small_available = "whisper_small_encoder" in model_manager.models

        print(f"   Whisper Large dispon√≠vel: {'‚úÖ' if large_available else '‚ùå'}")
        print(f"   Whisper Small dispon√≠vel: {'‚úÖ' if small_available else '‚ùå'}")

        if large_available:
            large_entry = model_manager.models["whisper_large_encoder"]
            print("   üìä Whisper Large:")
            print(f"      Path: {large_entry.path}")
            print(f"      Size: {large_entry.size_mb}MB")
            print(f"      Latency: {large_entry.latency_target_ms}ms")

        return large_available

    except Exception as e:
        print(f"‚ùå Erro no teste Whisper: {e}")
        return False

def test_anythingllm_integration():
    """Testa integra√ß√£o completa com AnythingLLM"""
    print("\nüß™ Testando Integra√ß√£o AnythingLLM...")

    try:
        from src.ai.llm_service import LLMService

        # Inicializar com AnythingLLM priorit√°rio
        service = LLMService(
            model_dir="models/",
            use_simulation=False,
            use_anythingllm=True
        )

        print("üöÄ Inicializando LLM Service...")
        success = service.initialize()

        if not success:
            print("‚ùå Falha na inicializa√ß√£o")
            return False

        print("‚úÖ LLM Service inicializado!")

        # Verificar status
        status = service.get_status()
        print("üìä Status dos servi√ßos:")
        print(f"   Simula√ß√£o: {status.get('use_simulation', True)}")
        print(f"   AnythingLLM: {status.get('anythingllm_available', False)}")

        # Testar resposta de vendas
        test_scenarios = [
            "Cliente disse: 'Est√° muito caro'",
            "Cliente: 'Quando podemos implementar?'",
            "Cliente: 'Como voc√™s se comparam com X?'",
            "Cliente: 'Preciso falar com meu chefe'"
        ]

        print("\nü§ñ Testando cen√°rios de vendas:")
        for i, prompt in enumerate(test_scenarios, 1):
            try:
                response = service.generate_response(prompt, max_tokens=150)
                print(f"   {i}. {prompt}")
                print(f"      ‚Üí {response[:80]}...")
                print()
            except Exception as e:
                print(f"   {i}. ‚ùå Erro: {e}")
                print()

        # Cleanup
        service.cleanup()
        return True

    except Exception as e:
        print(f"‚ùå Erro no teste AnythingLLM: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_summary_generation():
    """Testa gera√ß√£o de resumo com AnythingLLM"""
    print("\nüß™ Testando Gera√ß√£o de Resumo...")

    try:
        from src.core.summary_service import PostCallSummaryService, CallSummary
        from src.ai.llm_service import LLMService

        # Inicializar LLM Service
        llm_service = LLMService(use_anythingllm=True)
        llm_service.initialize()

        # Inicializar Summary Service
        config = type('Config', (), {'app_dir': Path('models')})()
        summary_service = PostCallSummaryService(config, llm_service)

        # Dados de teste simulados
        call_data = {
            "transcript": {
                "chunks": [
                    {"speaker": "vendor", "text": "Ola, bom dia! Como vai?", "duration_sec": 2.0},
                    {"speaker": "client", "text": "Oi, tudo bem. Gostaria de saber sobre a solucao", "duration_sec": 3.0},
                    {"speaker": "vendor", "text": "Claro! Nossa plataforma oferece integracao completa", "duration_sec": 4.0},
                    {"speaker": "client", "text": "Parece interessante, mas esta muito caro", "duration_sec": 2.5},
                    {"speaker": "vendor", "text": "Entendo sua preocupacao. O ROI e de 340% em 18 meses", "duration_sec": 5.0},
                    {"speaker": "client", "text": "Hmm, preciso pensar melhor", "duration_sec": 2.0}
                ],
                "text": "Transcri√ß√£o completa da chamada de vendas"
            },
            "sentiment_data": {"avg_score": 0.65},
            "objections": [{"type": "preco", "handled": True}],
            "metrics": {
                "talk_time_vendor_pct": 55.0,
                "talk_time_client_pct": 45.0,
                "sentiment_avg": 0.65,
                "objections_total": 1,
                "objections_resolved": 1,
                "buying_signals": 2
            }
        }

        print("üìù Gerando resumo da chamada...")
        summary = summary_service.generate("test_call_001")

        if summary:
            print("‚úÖ Resumo gerado com sucesso!")
            print("üìä Resultado:")
            print(f"   Pontos principais: {len(summary.key_points)}")
            print(f"   Obje√ß√µes: {len(summary.objections)}")
            print(f"   Pr√≥ximos passos: {len(summary.next_steps)}")

            print("\nüìã Conte√∫do do resumo:")
            print(f"   Pontos: {summary.key_points}")
            print(f"   M√©tricas: Talk time vendor: {summary.metrics['talk_time_vendor_pct']}%")

            # Tentar parsear insights se dispon√≠vel
            if hasattr(summary, 'insights') and summary.insights:
                print(f"   Insights: Interesse {summary.insights.get('cliente_interesse', 'N/A')}")

            return True
        else:
            print("‚ùå Falha na gera√ß√£o do resumo")
            return False

    except Exception as e:
        print(f"‚ùå Erro no teste de resumo: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Executa todos os testes"""
    print("üöÄ TESTE FINAL - PitchAI com Whisper Large + AnythingLLM")
    print("=" * 60)

    # Resultados
    results = {
        "whisper_large": False,
        "anythingllm": False,
        "summary": False
    }

    # Executar testes
    results["whisper_large"] = test_whisper_large_priority()
    results["anythingllm"] = test_anythingllm_integration()
    results["summary"] = test_summary_generation()

    # Resumo final
    print("\n" + "=" * 60)
    print("üìã RESULTADO DOS TESTES")
    print("=" * 60)

    total_tests = len(results)
    passed_tests = sum(results.values())

    for test_name, passed in results.items():
        status = "‚úÖ PASSOU" if passed else "‚ùå FALHOU"
        print(f"{test_name.replace('_', ' ').title()}: {status}")

    print("-" * 60)
    print(".1f")

    if passed_tests == total_tests:
        print("üéâ SUCESSO TOTAL!")
        print("üéØ PitchAI est√° pronto com:")
        print("   ‚Ä¢ Whisper Large para transcri√ß√£o de alta qualidade")
        print("   ‚Ä¢ AnythingLLM para an√°lise inteligente de vendas")
        print("   ‚Ä¢ Sistema de resumos automatizados")
        print("\nüí° Execute: python src/main_frontend.py")
    else:
        print("‚ö†Ô∏è Alguns testes falharam.")
        print("üîß Verifique os logs acima para detalhes.")

    print("=" * 60)

if __name__ == "__main__":
    main()
