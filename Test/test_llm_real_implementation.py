#!/usr/bin/env python3
"""
Teste da Implementa√ß√£o do LLM Service
=====================================

Testes que verificam se o LLM Service funciona corretamente,
usando tanto modo real quanto simulado.
"""

import pytest
import sys
import time
import logging
from pathlib import Path

# Adicionar src ao path
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from ai.llm_service import LLMService

class TestLLMRealImplementation:
    """Testes da implementa√ß√£o REAL do LLM."""
    
    def setup_method(self):
        """Setup para cada teste."""
        self.model_dir = Path(__file__).parent.parent / "models" / "llmware_model"
        logging.basicConfig(level=logging.INFO)
        
    def test_model_files_exist(self):
        """Teste REAL: Verificar se arquivos do modelo existem."""
        assert self.model_dir.exists(), f"Diret√≥rio do modelo n√£o existe: {self.model_dir}"
        
        # Arquivos essenciais para o modelo funcionar
        essential_files = [
            "genai_config.json",
            "config.json", 
            "tokenizer.json",
            "prompt_1_of_3_qnn_ctx.onnx",
            "token_1_of_3_qnn_ctx.onnx",
            "llama_v3_2_3b_chat_quantized_part_1_of_3.bin"
        ]
        
        missing_files = []
        for filename in essential_files:
            file_path = self.model_dir / filename
            if not file_path.exists():
                missing_files.append(filename)
        
        assert not missing_files, f"Arquivos essenciais faltando: {missing_files}"
        
    def test_llm_service_initialization_simulated(self):
        """Teste: Inicializa√ß√£o do servi√ßo LLM em modo simulado."""
        service = LLMService(str(self.model_dir), use_simulation=True)
        assert service.use_simulation
        
        success = service.initialize()
        assert success, "Servi√ßo simulado deveria conseguir inicializar"
        assert service.is_initialized, "Servi√ßo simulado deveria estar marcado como inicializado"

    @pytest.mark.skipif(not LLMService(None, use_simulation=False).use_simulation, reason="llmware n√£o dispon√≠vel")
    def test_llm_service_initialization_real(self):
        """Teste: Inicializa√ß√£o do servi√ßo LLM em modo real (requer llmware)."""
        service = LLMService(str(self.model_dir), use_simulation=False)
        assert not service.use_simulation
        
        success = service.initialize()
        assert success, "Servi√ßo real deveria conseguir inicializar"
        assert service.is_initialized, "Servi√ßo real deveria estar marcado como inicializado"
        service.cleanup()

    def test_simulated_text_generation(self):
        """Teste: Gera√ß√£o de texto em modo simulado."""
        service = LLMService(str(self.model_dir), use_simulation=True)
        service.initialize()
        
        prompt = "Qual o pre√ßo?"
        response = service.generate_response(prompt)
        
        assert response
        assert "pre√ßo" in response.lower() or "valor" in response.lower()
        print(f"‚úÖ Gera√ß√£o simulada: '{prompt}' ‚Üí '{response}'")

    def test_real_text_generation(self):
        """Teste: Gera√ß√£o de texto funcional em modo real."""
        service = LLMService(str(self.model_dir), use_simulation=False)
        if not service.initialize():
            pytest.skip("Falha ao inicializar servi√ßo em modo real")

        try:
            # Teste de gera√ß√£o
            prompt = "Como responder obje√ß√£o de pre√ßo?"

            start_time = time.time()
            response = service.generate_response(prompt, max_tokens=50)
            generation_time = time.time() - start_time

            # Verifica√ß√µes da resposta
            assert response, "Resposta n√£o pode ser vazia"
            assert len(response) > 10, f"Resposta muito curta: '{response}'"
            assert response != prompt, "Resposta n√£o pode ser igual ao prompt"
            assert not response.startswith("‚ùå"), f"Resposta com erro: {response}"

            # Verificar tempo de resposta razo√°vel
            assert generation_time < 30.0, f"Gera√ß√£o muito lenta: {generation_time:.2f}s"

            print(f"‚úÖ Gera√ß√£o: '{prompt}' ‚Üí '{response}' ({generation_time:.2f}s)")

        finally:
            if hasattr(service, 'cleanup'):
                service.cleanup()
    
    def test_multiple_generations(self):
        """Teste: M√∫ltiplas gera√ß√µes consecutivas em modo real."""
        service = LLMService(str(self.model_dir), use_simulation=False)
        if not service.initialize():
            pytest.skip("Falha ao inicializar servi√ßo em modo real")

        try:
            prompts = [
                "Pre√ßo alto demais",
                "N√£o temos or√ßamento",
                "Preciso falar com minha equipe",
                "E o suporte t√©cnico?"
            ]

            responses = []
            total_time = 0

            for prompt in prompts:
                start_time = time.time()
                response = service.generate_response(prompt, max_tokens=30)
                generation_time = time.time() - start_time

                # Verifica√ß√µes b√°sicas
                assert response, f"Resposta vazia para: {prompt}"
                assert len(response) > 5, f"Resposta muito curta para '{prompt}': '{response}'"

                responses.append(response)
                total_time += generation_time

                print(f"  {len(responses)}. '{prompt}' ‚Üí '{response}' ({generation_time:.2f}s)")

            # Verificar que todas as respostas s√£o diferentes
            unique_responses = set(responses)
            assert len(unique_responses) > 1, "Todas as respostas s√£o iguais (servi√ßo n√£o est√° funcionando)"

            print(f"‚úÖ {len(prompts)} gera√ß√µes em {total_time:.2f}s")

        finally:
            if hasattr(service, 'cleanup'):
                service.cleanup()
    
    def test_factory_function(self):
        """Teste: Fun√ß√£o de cria√ß√£o do servi√ßo."""
        # Criar servi√ßo
        service = LLMService(str(self.model_dir), use_simulation=True)

        assert service is not None, "Servi√ßo n√£o foi criado"

        # Verificar se conseguiu inicializar
        success = service.initialize()
        assert success, "Servi√ßo deve conseguir inicializar"

        # Testar que consegue gerar resposta
        response = service.generate_response("Teste de funcionamento")
        assert response, "Servi√ßo deve conseguir gerar resposta"
        assert len(response) > 5, "Resposta deve ter conte√∫do"

        print("‚úÖ Servi√ßo criado e funcionando")

        # Cleanup se necess√°rio
        if hasattr(service, 'cleanup'):
            service.cleanup()
    
    def test_performance_requirements(self):
        """Teste: Verificar requirements de performance em modo real."""
        service = LLMService(str(self.model_dir), use_simulation=False)
        if not service.initialize():
            pytest.skip("Falha ao inicializar servi√ßo em modo real")

        try:
            # Teste de lat√™ncia
            prompt = "Teste de performance"

            # M√∫ltiplas medi√ß√µes para m√©dia
            times = []
            for i in range(3):
                start_time = time.time()
                response = service.generate_response(prompt, max_tokens=20)
                generation_time = time.time() - start_time
                times.append(generation_time)

                assert response, f"Resposta {i+1} vazia"

            avg_time = sum(times) / len(times)
            min_time = min(times)
            max_time = max(times)

            print(f"‚è±Ô∏è Lat√™ncia - M√©dia: {avg_time:.2f}s, Min: {min_time:.2f}s, Max: {max_time:.2f}s")

            # Requirements de lat√™ncia (ajustar conforme necess√°rio)
            assert avg_time < 5.0, f"Lat√™ncia m√©dia muito alta: {avg_time:.2f}s"
            assert min_time < 10.0, f"Lat√™ncia m√≠nima muito alta: {min_time:.2f}s"

        finally:
            if hasattr(service, 'cleanup'):
                service.cleanup()
    
    def test_sales_context_responses(self):
        """Teste: Verificar se respostas s√£o apropriadas para vendas em modo real."""
        service = LLMService(str(self.model_dir), use_simulation=False)
        if not service.initialize():
            pytest.skip("Falha ao inicializar servi√ßo em modo real")

        try:
            sales_scenarios = [
                ("Est√° muito caro", ["pre√ßo", "custo", "valor", "investimento", "roi"]),
                ("N√£o temos tempo", ["prazo", "tempo", "implementa√ß√£o", "r√°pido"]),
                ("E o suporte?", ["suporte", "ajuda", "assist√™ncia", "t√©cnico"]),
                ("Como funciona?", ["funciona", "processo", "como", "m√©todo"])
            ]

            for scenario, expected_words in sales_scenarios:
                response = service.generate_response(scenario, max_tokens=50)

                assert response, f"Resposta vazia para: {scenario}"

                # Verificar se pelo menos uma palavra-chave est√° presente
                response_lower = response.lower()
                has_relevant_word = any(word in response_lower for word in expected_words)

                print(f"üéØ '{scenario}' ‚Üí '{response}'")
                print(f"   Palavras esperadas: {expected_words}")
                print(f"   Relevante: {'‚úÖ' if has_relevant_word else '‚ùå'}")

                # N√£o falha o teste se n√£o tiver palavra exata, mas registra
                if not has_relevant_word:
                    print(f"‚ö†Ô∏è Resposta pode n√£o estar totalmente alinhada com contexto de vendas")

        finally:
            if hasattr(service, 'cleanup'):
                service.cleanup()

# Teste de integra√ß√£o completa
def test_complete_integration_simulated():
    """Teste de integra√ß√£o completa do sistema em modo simulado."""
    print("\nüîÑ Teste de Integra√ß√£o Completa (Simulado)")
    print("=" * 50)

    model_dir = Path(__file__).parent.parent / "models" / "llmware_model"

    # 1. Verificar arquivos
    print("1. Verificando arquivos...")
    assert model_dir.exists(), "Diret√≥rio do modelo deve existir"

    # 2. Tentar criar servi√ßo
    print("2. Criando servi√ßo...")
    from ai.llm_service import LLMService
    service = LLMService(str(model_dir), use_simulation=True)
    assert service is not None, "Servi√ßo deve ser criado"

    # 3. Inicializar
    print("3. Inicializando servi√ßo...")
    success = service.initialize()
    assert success, "Servi√ßo deve conseguir inicializar"

    # 4. Testar gera√ß√£o
    print("4. Testando gera√ß√£o...")
    response = service.generate_response("Como est√° voc√™ hoje?")
    assert response, "Deve gerar resposta"
    print(f"   Resposta: '{response}'")

    # 5. Cleanup
    print("5. Limpando recursos...")
    if hasattr(service, 'cleanup'):
        service.cleanup()

    print("‚úÖ Integra√ß√£o completa (simulada) testada com sucesso!")

if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.INFO)

    # Executar teste de integra√ß√£o
    test_complete_integration_simulated()

    # Executar testes unit√°rios
    print("\nüß™ Executando testes unit√°rios...")
    test_instance = TestLLMRealImplementation()
    test_instance.setup_method()

    try:
        test_instance.test_model_files_exist()
        print("‚úÖ test_model_files_exist")

        test_instance.test_llm_service_initialization_simulated()
        print("‚úÖ test_llm_service_initialization_simulated")
        
        test_instance.test_simulated_text_generation()
        print("‚úÖ test_simulated_text_generation")

        # Testes que podem falhar em diferentes ambientes
        try:
            test_instance.test_llm_service_initialization_real()
            print("‚úÖ test_llm_service_initialization_real")
        except Exception as e:
            print(f"‚ö†Ô∏è test_llm_service_initialization_real: {e}")

        try:
            test_instance.test_real_text_generation()
            print("‚úÖ test_real_text_generation")
        except Exception as e:
            print(f"‚ö†Ô∏è test_real_text_generation: {e}")

        try:
            test_instance.test_multiple_generations()
            print("‚úÖ test_multiple_generations")
        except Exception as e:
            print(f"‚ö†Ô∏è test_multiple_generations: {e}")

        try:
            test_instance.test_performance_requirements()
            print("‚úÖ test_performance_requirements")
        except Exception as e:
            print(f"‚ö†Ô∏è test_performance_requirements: {e}")

        try:
            test_instance.test_sales_context_responses()
            print("‚úÖ test_sales_context_responses")
        except Exception as e:
            print(f"‚ö†Ô∏è test_sales_context_responses: {e}")

        print("\nüéâ Todos os testes executados!")

    except Exception as e:
        print(f"\n‚ùå Teste falhou: {e}")
        import traceback
        traceback.print_exc()
