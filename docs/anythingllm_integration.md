# IntegraÃ§Ã£o AnythingLLM no PitchAI

## ğŸ“‹ VisÃ£o Geral

Este documento descreve a integraÃ§Ã£o do **AnythingLLM** como o "cÃ©rebro" do PitchAI, operando 100% offline com modelos locais na NPU.

## ğŸ¯ Objetivo

- **Substituir simulaÃ§Ãµes** por IA real e inteligente
- **100% offline** - nenhum dado sai do dispositivo
- **NPU-first** - aproveitamento mÃ¡ximo do hardware Snapdragon X+
- **Fallback robusto** - mÃºltiplas camadas de backup

---

## ğŸ—ï¸ Arquitetura da IntegraÃ§Ã£o

```mermaid
graph TD
    A[PitchAI LLM Service] --> B{AnythingLLM Client}
    B --> C{Offline Mode}
    B --> D{Online Mode - Fallback}

    C --> E[Llama 3.2 3B Local]
    C --> F[Embeddings Locais]
    C --> G[Cache Inteligente]

    D --> H[API AnythingLLM]
    D --> I[Modelo Remoto]

    E --> J[Respostas Contextuais]
    F --> K[RAG com Base Local]
    G --> L[Performance Otimizada]
```

---

## ğŸ“¦ DependÃªncias NecessÃ¡rias

### Core Dependencies
```python
# requirements.txt
anythingllm-client>=0.1.0
requests>=2.31.0
numpy>=1.24.0
torch>=2.0.0  # Para processamento local
transformers>=4.30.0
```

### System Requirements
- **Python**: 3.11+
- **RAM**: 8GB+ (para modelos Llama 3.2 3B)
- **Storage**: 4GB+ para modelos
- **NPU**: Snapdragon X+ (QNN Execution Provider)

---

## âš™ï¸ ConfiguraÃ§Ã£o

### 1. ConfiguraÃ§Ã£o BÃ¡sica

```python
from src.ai.anythingllm_client import AnythingLLMClient, AnythingLLMConfig

# ConfiguraÃ§Ã£o offline (recomendada)
config = AnythingLLMConfig(
    base_url="http://localhost:3001",  # NÃ£o usado em offline
    workspace_name="pitchai_workspace",
    model_name="llama-3.2-3b-instruct",
    offline_mode=True,  # Sempre True para PitchAI
    timeout_seconds=30,
    max_retries=3
)

client = AnythingLLMClient(config)
success = client.initialize()
```

### 2. Modelos NecessÃ¡rios

#### Llama 3.2 3B Instruct (Principal)
- **LocalizaÃ§Ã£o**: `models/llama-3.2-3b-onnx-qnn/`
- **Formato**: ONNX QNN otimizado
- **Tamanho**: ~2GB
- **Contexto**: 4096 tokens
- **Uso**: GeraÃ§Ã£o de respostas inteligentes

#### Modelo de Embeddings
- **LocalizaÃ§Ã£o**: `models/embeddings/`
- **DimensÃ£o**: 384
- **Uso**: RAG e busca semÃ¢ntica

### 3. Estrutura de DiretÃ³rios

```
models/
â”œâ”€â”€ llama-3.2-3b-onnx-qnn/
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â””â”€â”€ *.bin (partes do modelo)
â”œâ”€â”€ embeddings/
â”‚   â””â”€â”€ model.onnx
â””â”€â”€ manifest.json  # Atualizado com configuraÃ§Ãµes
```

---

## ğŸš€ Como Usar

### InicializaÃ§Ã£o BÃ¡sica

```python
from src.ai.llm_service import LLMService

# Inicializar com AnythingLLM prioritÃ¡rio
service = LLMService(
    model_dir="models/",
    use_simulation=False,
    use_anythingllm=True
)

if service.initialize():
    print("âœ… AnythingLLM pronto!")
else:
    print("âš ï¸ Usando modo fallback")
```

### GeraÃ§Ã£o de Respostas

```python
# Exemplo: Resposta para objeÃ§Ã£o de preÃ§o
prompt = "Cliente disse: 'EstÃ¡ muito caro'. Como responder?"

response = service.generate_response(
    prompt=prompt,
    max_tokens=256,
    include_history=True
)

print(f"ğŸ¤– SugestÃ£o: {response}")
```

### Monitoramento

```python
# Verificar status
status = service.get_status()
print(f"Status: {status}")

# EstatÃ­sticas de uso
if 'anythingllm_stats' in status:
    stats = status['anythingllm_stats']
    print(f"Cache Hit Rate: {stats.get('cache_hit_rate', 0):.1%}")
    print(f"Avg Response Time: {stats.get('avg_response_time', 0):.1f}ms")
```

---

## ğŸ¯ Funcionalidades Implementadas

### 1. Modo Offline Completo
- âœ… Modelos locais (Llama 3.2 3B)
- âœ… Embeddings locais para RAG
- âœ… Cache inteligente de respostas
- âœ… Processamento 100% no dispositivo

### 2. EstratÃ©gia de Fallback
1. **AnythingLLM Offline** (prioridade)
2. **LLMWare** (fallback)
3. **SimulaÃ§Ã£o AvanÃ§ada** (Ãºltimo recurso)

### 3. OtimizaÃ§Ãµes de Performance
- **NPU-first**: ExecuÃ§Ã£o preferencial na NPU
- **Cache inteligente**: Respostas similares reutilizadas
- **Processamento assÃ­ncrono**: NÃ£o bloqueia UI
- **CompressÃ£o de contexto**: HistÃ³rico otimizado

### 4. Contexto de Vendas
O AnythingLLM foi treinado com contexto especÃ­fico de vendas:

```python
# Exemplos de prompts otimizados
prompts = {
    "preco": "ğŸ’° EstratÃ©gia para objeÃ§Ã£o de preÃ§o...",
    "prazo": "â° Tratamento de preocupaÃ§Ã£o com prazo...",
    "concorrente": "ğŸ¯ Posicionamento competitivo...",
    "geral": "ğŸ¤– Assistente de vendas inteligente..."
}
```

---

## ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

### Cache Configuration

```python
# Configurar cache
client = AnythingLLMClient(config)
client.cache_max_size = 200  # Mais respostas em cache
client.cache_expiry_hours = 24  # Validade do cache
```

### Performance Tuning

```python
# OtimizaÃ§Ãµes para NPU
config = AnythingLLMConfig(
    offline_mode=True,
    max_tokens=512,  # Limitar para performance
    temperature=0.7,  # Criatividade balanceada
    timeout_seconds=15  # Timeout reduzido
)
```

### Logging Detalhado

```python
import logging
logging.basicConfig(level=logging.DEBUG)

# Logs incluem:
# - Tempo de resposta
# - ConfianÃ§a do modelo
# - Cache hits/misses
# - Erros e fallbacks
```

---

## ğŸ“Š Monitoramento e MÃ©tricas

### MÃ©tricas Principais

```python
stats = client.get_stats()
print(f"""
ğŸ“Š EstatÃ­sticas AnythingLLM:
- Total de requests: {stats['requests_total']}
- Taxa de sucesso: {stats['success_rate']:.1%}
- Cache hit rate: {stats['cache_hit_rate']:.1%}
- Tempo mÃ©dio: {stats.get('avg_response_time', 0):.1f}ms
""")
```

### Health Checks

```python
# Verificar saÃºde do serviÃ§o
health = client.is_connected
if not health:
    print("âŒ AnythingLLM desconectado")
    # Sistema automaticamente usa fallback
```

---

## ğŸ› ï¸ Troubleshooting

### Problemas Comuns

#### 1. Modelo NÃ£o Carrega
```bash
# Verificar arquivos
ls -la models/llama-3.2-3b-onnx-qnn/

# Verificar logs
tail -f logs/pitchai.log | grep -i anythingllm
```

#### 2. Performance Lenta
```python
# Verificar se NPU estÃ¡ sendo usada
status = service.get_status()
if 'npu_used' in status:
    print(f"NPU ativa: {status['npu_used']}")
```

#### 3. Cache NÃ£o Funciona
```python
# Limpar cache manualmente
client.clear_cache()
print("ğŸ§¹ Cache limpo")
```

### Logs de Debug

```python
# Ativar debug detalhado
import logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Logs incluem traces completos do AnythingLLM
```

---

## ğŸ”„ AtualizaÃ§Ãµes e ManutenÃ§Ã£o

### AtualizaÃ§Ã£o de Modelos

```bash
# 1. Backup do modelo atual
cp -r models/llama-3.2-3b-onnx-qnn/ backup/

# 2. Substituir arquivos
# ... colocar novos arquivos ONNX ...

# 3. Reinicializar serviÃ§o
service.cleanup()
service.initialize()
```

### Monitoramento ContÃ­nuo

```python
# Script de health check
def check_anythingllm_health():
    try:
        response = service.generate_response("Teste de saÃºde")
        return len(response) > 10
    except:
        return False

# Executar periodicamente
if not check_anythingllm_health():
    print("âš ï¸ AnythingLLM precisa de atenÃ§Ã£o")
```

---

## ğŸ“š ReferÃªncias

- [AnythingLLM Documentation](https://docs.useanything.com/)
- [ONNX Runtime QNN Provider](https://onnxruntime.ai/docs/execution-providers/QNN-ExecutionProvider.html)
- [Llama 3.2 Model](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)

---

## ğŸ¯ ConclusÃ£o

A integraÃ§Ã£o do **AnythingLLM** transforma o PitchAI de um sistema de simulaÃ§Ã£o para uma **IA real e inteligente**, mantendo o compromisso com:

- **100% Offline** ğŸ“´
- **NPU Otimizado** ğŸš€
- **Performance Superior** âš¡
- **Contexto de Vendas** ğŸ’¼
- **Fallback Robusto** ğŸ›¡ï¸

O resultado Ã© um copiloto de vendas verdadeiramente inteligente que opera nativamente no Windows com Snapdragon X+.
