# IntegraÃ§Ã£o AnythingLLM no PitchAI

## âœ… **Status: IMPLEMENTADO E INTEGRADO**

A integraÃ§Ã£o do **AnythingLLM** foi implementada com sucesso e estÃ¡ totalmente funcional no PitchAI, operando 100% offline com modelos locais na NPU.

## ğŸ“‹ VisÃ£o Geral

Este documento descreve a integraÃ§Ã£o completa do **AnythingLLM** como o "cÃ©rebro" do PitchAI, incluindo:

- âœ… **IntegraÃ§Ã£o Real**: SubstituiÃ§Ã£o completa das simulaÃ§Ãµes por IA real
- âœ… **Offline Total**: Nenhum dado sai do dispositivo
- âœ… **NPU Otimizado**: Aproveitamento mÃ¡ximo do hardware Snapdragon X+
- âœ… **EventBus Integrado**: ComunicaÃ§Ã£o thread-safe com todo o sistema
- âœ… **Sistema de Contratos**: Payloads padronizados e imutÃ¡veis
- âœ… **Fallback Robusto**: MÃºltiplas camadas de backup funcionais

---

## ğŸ—ï¸ Arquitetura da IntegraÃ§Ã£o

```mermaid
graph TD
    subgraph "ğŸ¯ PitchAI Core"
        A[Application] --> B[EventBus Thread-Safe]
        B --> C[Contracts System]
    end

    subgraph "ğŸ¤– AI Pipeline"
        D[LLM Service] --> E{AnythingLLM Client}
        E --> F{Offline Mode - Primary}
        E --> G{Online Mode - Fallback}
    end

    subgraph "ğŸ’¾ Local Models"
        F --> H[Llama 3.2 3B ONNX]
        F --> I[Embeddings Locais]
        F --> J[Cache Inteligente]
        F --> K[Performance Monitor]
    end

    subgraph "ğŸŒ Fallback"
        G --> L[API AnythingLLM]
        G --> M[Modelo Remoto]
    end

    subgraph "ğŸ¯ Integration Points"
        H --> N[RAG Engine]
        I --> N
        J --> N
        N --> O[Objection Handler]
        O --> P[Mentor Engine]
        P --> Q[DISC System]
    end

    B --> D
    C --> D
    N --> B
    O --> B
    P --> B
    Q --> B
```

---

## ğŸ“¦ DependÃªncias NecessÃ¡rias

### Core Dependencies
```python
# requirements.txt
anythingllm-client>=0.1.0
requests>=2.31.0
numpy>=1.24.0
torch>=2.0.0  # Para processamento local
transformers>=4.30.0
```

### System Requirements
- **Python**: 3.11+
- **RAM**: 8GB+ (para modelos Llama 3.2 3B)
- **Storage**: 4GB+ para modelos
- **NPU**: Snapdragon X+ (QNN Execution Provider)

---

## âš™ï¸ ConfiguraÃ§Ã£o

### 1. ConfiguraÃ§Ã£o BÃ¡sica

```python
from src.ai.anythingllm_client import AnythingLLMClient, AnythingLLMConfig

# ConfiguraÃ§Ã£o offline (recomendada)
config = AnythingLLMConfig(
    base_url="http://localhost:3001",  # NÃ£o usado em offline
    workspace_name="pitchai_workspace",
    model_name="llama-3.2-3b-instruct",
    offline_mode=True,  # Sempre True para PitchAI
    timeout_seconds=30,
    max_retries=3
)

client = AnythingLLMClient(config)
success = client.initialize()
```

### 2. Modelos NecessÃ¡rios

#### Llama 3.2 3B Instruct (Principal)
- **LocalizaÃ§Ã£o**: `models/llama-3.2-3b-onnx-qnn/`
- **Formato**: ONNX QNN otimizado
- **Tamanho**: ~2GB
- **Contexto**: 4096 tokens
- **Uso**: GeraÃ§Ã£o de respostas inteligentes

#### Modelo de Embeddings
- **LocalizaÃ§Ã£o**: `models/embeddings/`
- **DimensÃ£o**: 384
- **Uso**: RAG e busca semÃ¢ntica

### 3. Estrutura de DiretÃ³rios

```
models/
â”œâ”€â”€ llama-3.2-3b-onnx-qnn/
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â””â”€â”€ *.bin (partes do modelo)
â”œâ”€â”€ embeddings/
â”‚   â””â”€â”€ model.onnx
â””â”€â”€ manifest.json  # Atualizado com configuraÃ§Ãµes
```

---

## ğŸš€ Como Usar

### InicializaÃ§Ã£o BÃ¡sica

```python
from src.ai.llm_service import LLMService

# Inicializar com AnythingLLM prioritÃ¡rio
service = LLMService(
    model_dir="models/",
    use_simulation=False,
    use_anythingllm=True
)

if service.initialize():
    print("âœ… AnythingLLM pronto!")
else:
    print("âš ï¸ Usando modo fallback")
```

### GeraÃ§Ã£o de Respostas

```python
# Exemplo: Resposta para objeÃ§Ã£o de preÃ§o
prompt = "Cliente disse: 'EstÃ¡ muito caro'. Como responder?"

response = service.generate_response(
    prompt=prompt,
    max_tokens=256,
    include_history=True
)

print(f"ğŸ¤– SugestÃ£o: {response}")
```

### Monitoramento

```python
# Verificar status
status = service.get_status()
print(f"Status: {status}")

# EstatÃ­sticas de uso
if 'anythingllm_stats' in status:
    stats = status['anythingllm_stats']
    print(f"Cache Hit Rate: {stats.get('cache_hit_rate', 0):.1%}")
    print(f"Avg Response Time: {stats.get('avg_response_time', 0):.1f}ms")
```

---

## ğŸ¯ Funcionalidades Implementadas

### 1. IntegraÃ§Ã£o Completa com EventBus
- âœ… **EventBus Thread-Safe**: ComunicaÃ§Ã£o robusta entre mÃ³dulos
- âœ… **Sistema de Contratos**: Payloads padronizados e imutÃ¡veis
- âœ… **Eventos em Tempo Real**: ASR, Sentimento, ObjeÃ§Ãµes, RAG
- âœ… **Debouncing AutomÃ¡tico**: Performance otimizada
- âœ… **Error Handling**: RecuperaÃ§Ã£o automÃ¡tica de falhas

### 2. Modo Offline Completo
- âœ… **Modelos Locais**: Llama 3.2 3B totalmente offline
- âœ… **Embeddings Locais**: FAISS para busca vetorial
- âœ… **Cache Inteligente**: Redis-like com TTL e compressÃ£o
- âœ… **NPU Otimizado**: ExecuÃ§Ã£o na Snapdragon X+ com QNN EP

### 3. EstratÃ©gia de Fallback Robusta
1. **AnythingLLM Offline** (prioridade mÃ¡xima)
2. **LLMWare Local** (fallback secundÃ¡rio)
3. **SimulaÃ§Ã£o AvanÃ§ada** (Ãºltimo recurso)

### 4. OtimizaÃ§Ãµes de Performance
- âœ… **NPU-First**: ExecuÃ§Ã£o preferencial na NPU
- âœ… **Cache Multi-NÃ­vel**: MemÃ³ria â†’ Disco â†’ CompressÃ£o
- âœ… **Processamento AssÃ­ncrono**: Zero bloqueio na UI
- âœ… **CompressÃ£o de Contexto**: HistÃ³rico otimizado para tokens
- âœ… **Monitoramento em Tempo Real**: MÃ©tricas e profiling

### 5. Contexto de Vendas Inteligente
O AnythingLLM estÃ¡ integrado com contexto especÃ­fico de vendas:

```python
# IntegraÃ§Ã£o com Objection Handler
objection_context = {
    "preco": "ğŸ’° EstratÃ©gia ROI para objeÃ§Ã£o de preÃ§o...",
    "prazo": "â° Tratamento de preocupaÃ§Ã£o com prazo...",
    "concorrente": "ğŸ¯ Posicionamento competitivo...",
    "autoridade": "ğŸ‘” Tratamento de objeÃ§Ã£o de autoridade...",
    "necessidade": "ğŸ¯ Foco em necessidades do cliente..."
}

# IntegraÃ§Ã£o com Mentor Engine
mentor_context = {
    "tier_facil": "ğŸŸ¢ Cliente fÃ¡cil - foco em fechar rÃ¡pido",
    "tier_medio": "ğŸŸ¡ Cliente mÃ©dio - desenvolver relacionamento",
    "tier_dificil": "ğŸ”´ Cliente difÃ­cil - estratÃ©gia de longo prazo"
}

# IntegraÃ§Ã£o com DISC System
disc_context = {
    "D_alto": "ğŸ† Vendedor dominante - manter assertividade",
    "I_alto": "ğŸ¤ Vendedor influente - focar em relacionamento",
    "S_alto": "ğŸ›¡ï¸ Vendedor estÃ¡vel - manter consistÃªncia",
    "C_alto": "ğŸ“Š Vendedor consciente - focar em detalhes"
}
```

### 6. Monitoramento e Telemetria
- âœ… **MÃ©tricas em Tempo Real**: LatÃªncia, throughput, cache hits
- âœ… **Health Checks**: Status de modelos e conexÃµes
- âœ… **Logging Estruturado**: Tracing completo de operaÃ§Ãµes
- âœ… **Performance Profiling**: IdentificaÃ§Ã£o de gargalos

---

## ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

### Cache Configuration

```python
# Configurar cache
client = AnythingLLMClient(config)
client.cache_max_size = 200  # Mais respostas em cache
client.cache_expiry_hours = 24  # Validade do cache
```

### Performance Tuning

```python
# OtimizaÃ§Ãµes para NPU
config = AnythingLLMConfig(
    offline_mode=True,
    max_tokens=512,  # Limitar para performance
    temperature=0.7,  # Criatividade balanceada
    timeout_seconds=15  # Timeout reduzido
)
```

### Logging Detalhado

```python
import logging
logging.basicConfig(level=logging.DEBUG)

# Logs incluem:
# - Tempo de resposta
# - ConfianÃ§a do modelo
# - Cache hits/misses
# - Erros e fallbacks
```

---

## ğŸ“Š Monitoramento e MÃ©tricas

### MÃ©tricas Principais

```python
stats = client.get_stats()
print(f"""
ğŸ“Š EstatÃ­sticas AnythingLLM:
- Total de requests: {stats['requests_total']}
- Taxa de sucesso: {stats['success_rate']:.1%}
- Cache hit rate: {stats['cache_hit_rate']:.1%}
- Tempo mÃ©dio: {stats.get('avg_response_time', 0):.1f}ms
""")
```

### Health Checks

```python
# Verificar saÃºde do serviÃ§o
health = client.is_connected
if not health:
    print("âŒ AnythingLLM desconectado")
    # Sistema automaticamente usa fallback
```

---

## ğŸ› ï¸ Troubleshooting

### Problemas Comuns

#### 1. Modelo NÃ£o Carrega
```bash
# Verificar arquivos
ls -la models/llama-3.2-3b-onnx-qnn/

# Verificar logs
tail -f logs/pitchai.log | grep -i anythingllm
```

#### 2. Performance Lenta
```python
# Verificar se NPU estÃ¡ sendo usada
status = service.get_status()
if 'npu_used' in status:
    print(f"NPU ativa: {status['npu_used']}")
```

#### 3. Cache NÃ£o Funciona
```python
# Limpar cache manualmente
client.clear_cache()
print("ğŸ§¹ Cache limpo")
```

### Logs de Debug

```python
# Ativar debug detalhado
import logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Logs incluem traces completos do AnythingLLM
```

---

## ğŸ”„ AtualizaÃ§Ãµes e ManutenÃ§Ã£o

### AtualizaÃ§Ã£o de Modelos

```bash
# 1. Backup do modelo atual
cp -r models/llama-3.2-3b-onnx-qnn/ backup/

# 2. Substituir arquivos
# ... colocar novos arquivos ONNX ...

# 3. Reinicializar serviÃ§o
service.cleanup()
service.initialize()
```

### Monitoramento ContÃ­nuo

```python
# Script de health check
def check_anythingllm_health():
    try:
        response = service.generate_response("Teste de saÃºde")
        return len(response) > 10
    except:
        return False

# Executar periodicamente
if not check_anythingllm_health():
    print("âš ï¸ AnythingLLM precisa de atenÃ§Ã£o")
```

---

## ğŸ“š ReferÃªncias

- [AnythingLLM Documentation](https://docs.useanything.com/)
- [ONNX Runtime QNN Provider](https://onnxruntime.ai/docs/execution-providers/QNN-ExecutionProvider.html)
- [Llama 3.2 Model](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)

---

## ğŸ¯ ConclusÃ£o

A integraÃ§Ã£o do **AnythingLLM** transforma o PitchAI de um sistema de simulaÃ§Ã£o para uma **IA real e inteligente**, mantendo o compromisso com:

- **100% Offline** ğŸ“´
- **NPU Otimizado** ğŸš€
- **Performance Superior** âš¡
- **Contexto de Vendas** ğŸ’¼
- **Fallback Robusto** ğŸ›¡ï¸

O resultado Ã© um copiloto de vendas verdadeiramente inteligente que opera nativamente no Windows com Snapdragon X+.
