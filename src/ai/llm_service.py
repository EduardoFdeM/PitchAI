"""
LLM Service - Servi√ßo Unificado para Llama 3.2 3B
=================================================

Servi√ßo principal para intera√ß√£o com o modelo Llama 3.2 3B,
usando a integra√ß√£o LLMWare espec√≠fica do PitchAI.
"""

import logging
import sys
from pathlib import Path
from typing import Optional, Dict, Any

# Importar a integra√ß√£o LLMWare espec√≠fica
try:
    sys.path.append(str(Path(__file__).parent.parent.parent / "models"))
    from llmware_model.pitchai_llmware_integration import LLMWareService
    LLMWARE_AVAILABLE = True
except ImportError as e:
    print(f"‚ö†Ô∏è LLMWare n√£o dispon√≠vel: {e}")
    LLMWARE_AVAILABLE = False

class LLMService:
    """Servi√ßo unificado para o modelo Llama 3.2 3B usando LLMWare."""

    def __init__(self, model_dir: str = None, use_simulation: bool = False):
        """
        Inicializa o servi√ßo LLM.
        
        Args:
            model_dir: Diret√≥rio do modelo (usado para localizar o modelo LLMWare)
            use_simulation: Se True, usa simula√ß√£o; se False, tenta usar o modelo real
        """
        self.model_dir = model_dir
        self.use_simulation = use_simulation
        self.logger = logging.getLogger(__name__)
        
        self.llmware_service = None
        self.is_initialized = False
        self.conversation_history = []

    def initialize(self) -> bool:
        """Inicializa o servi√ßo LLM."""
        try:
            if self.use_simulation:
                self.logger.info("üîÑ Inicializando LLM em modo simulado")
                self.is_initialized = True
                return True
            
            # Tentar inicializar o modelo real primeiro
            self.logger.info("üîÑ Inicializando LLMWare Service...")
            
            # A integra√ß√£o LLMWare est√° sempre dispon√≠vel (importa√ß√£o foi bem-sucedida)
            self.llmware_service = LLMWareService()
            
            if self.llmware_service.initialize():
                self.is_initialized = True
                self.logger.info("‚úÖ LLMWare Service inicializado com sucesso!")
                return True
            else:
                self.logger.warning("‚ùå Falha ao inicializar LLMWare Service, usando simula√ß√£o")
                # Fallback para simula√ß√£o
                self.use_simulation = True
                self.is_initialized = True
                return True
                
        except Exception as e:
            self.logger.error(f"‚ùå Erro na inicializa√ß√£o: {e}")
            # Fallback para simula√ß√£o em caso de erro
            self.use_simulation = True
            self.is_initialized = True
            return True

    def generate_response(self, prompt: str, max_tokens: int = 256, include_history: bool = True) -> str:
        """
        Gera resposta usando o modelo.
        
        Args:
            prompt: Texto de entrada
            max_tokens: N√∫mero m√°ximo de tokens para gerar
            include_history: Se deve incluir hist√≥rico da conversa
            
        Returns:
            Resposta gerada pelo modelo
        """
        if not self.is_initialized:
            return "‚ùå Servi√ßo n√£o inicializado"

        try:
            # Construir prompt com hist√≥rico se solicitado
            full_prompt = self._build_prompt_with_history(prompt) if include_history else prompt
            
            if self.use_simulation:
                response = self._generate_simulation_response(prompt)
            else:
                response = self.llmware_service.generate_response(full_prompt, max_tokens)
            
            # Adicionar ao hist√≥rico
            if include_history:
                self.conversation_history.append({"user": prompt, "assistant": response})
                # Limitar hist√≥rico para n√£o crescer indefinidamente
                if len(self.conversation_history) > 10:
                    self.conversation_history = self.conversation_history[-10:]
            
            return response
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro na gera√ß√£o: {e}")
            return f"‚ùå Erro: {str(e)}"

    def _build_prompt_with_history(self, prompt: str) -> str:
        """Constr√≥i prompt incluindo hist√≥rico da conversa."""
        if not self.conversation_history:
            return prompt
        
        history_text = ""
        for entry in self.conversation_history[-5:]:  # √öltimas 5 intera√ß√µes
            history_text += f"Usu√°rio: {entry['user']}\nAssistente: {entry['assistant']}\n\n"
        
        return f"Hist√≥rico da conversa:\n{history_text}Nova pergunta: {prompt}"

    def _generate_simulation_response(self, prompt: str) -> str:
        """Gera resposta simulada para desenvolvimento/teste."""
        import random
        
        responses = [
            f"üí° Baseado em '{prompt[:30]}...', sugiro abordar o ROI direto.",
            f"üéØ Para a situa√ß√£o mencionada, recomendo focar nos benef√≠cios de longo prazo.",
            f"üìä Analisando o contexto, a melhor estrat√©gia seria evidenciar o valor agregado.",
            f"üîç Considerando a obje√ß√£o, sugiro apresentar cases de sucesso similares.",
            f"‚ö° Para superar essa barreira, foque na urg√™ncia da solu√ß√£o."
        ]
        
        base_response = random.choice(responses)
        return f"{base_response}\n\nü§ñ (Resposta simulada - modelo real n√£o carregado)"

    def clear_history(self):
        """Limpa o hist√≥rico da conversa."""
        self.conversation_history = []
        self.logger.info("üßπ Hist√≥rico da conversa limpo")

    def cleanup(self):
        """Limpa recursos do servi√ßo."""
        try:
            if self.llmware_service and hasattr(self.llmware_service, 'cleanup'):
                self.llmware_service.cleanup()
            
            self.clear_history()
            self.is_initialized = False
            self.logger.info("üßπ LLMService limpo")
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro na limpeza: {e}")

    def get_status(self) -> Dict[str, Any]:
        """Retorna status do servi√ßo."""
        return {
            "is_initialized": self.is_initialized,
            "use_simulation": self.use_simulation,
            "llmware_available": LLMWARE_AVAILABLE,
            "conversation_length": len(self.conversation_history)
        }


