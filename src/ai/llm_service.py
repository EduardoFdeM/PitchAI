"""
LLM Service - Servi√ßo Unificado para Llama 3.2 3B
=================================================

Servi√ßo principal para intera√ß√£o com o modelo Llama 3.2 3B,
usando a integra√ß√£o LLMWare espec√≠fica do PitchAI.
"""

import logging
import sys
from pathlib import Path
from typing import Optional, Dict, Any

# Importar integra√ß√µes LLM
try:
    from .anythingllm_client import AnythingLLMClient, AnythingLLMConfig
    ANYTHINGLLM_AVAILABLE = True
except ImportError as e:
    print(f"‚ö†Ô∏è AnythingLLM Client n√£o dispon√≠vel: {e}")
    ANYTHINGLLM_AVAILABLE = False
    AnythingLLMClient = None

# Importar a integra√ß√£o LLMWare espec√≠fica (fallback)
try:
    sys.path.append(str(Path(__file__).parent.parent.parent / "models"))
    from llmware_model.pitchai_llmware_integration import LLMWareService
    LLMWARE_AVAILABLE = True
except ImportError as e:
    print(f"‚ö†Ô∏è LLMWare n√£o dispon√≠vel: {e}")
    LLMWARE_AVAILABLE = False
    LLMWareService = None

class LLMService:
    """Servi√ßo unificado para modelos LLM usando AnythingLLM como prioridade."""

    def __init__(self, model_dir: str = None, use_simulation: bool = False, use_anythingllm: bool = True):
        """
        Inicializa o servi√ßo LLM.

        Args:
            model_dir: Diret√≥rio do modelo (usado para localizar modelos)
            use_simulation: Se True, usa simula√ß√£o; se False, tenta usar modelos reais
            use_anythingllm: Se True, prioriza AnythingLLM sobre outros providers
        """
        self.model_dir = model_dir
        self.use_simulation = use_simulation
        self.use_anythingllm = use_anythingllm
        self.logger = logging.getLogger(__name__)

        # Servi√ßos dispon√≠veis
        self.anythingllm_client = None
        self.llmware_service = None

        self.is_initialized = False
        self.conversation_history = []

        # Configura√ß√µes
        self.anythingllm_config = AnythingLLMConfig(
            offline_mode=True,  # Sempre offline para PitchAI
            model_name="llama-3.2-3b-instruct"
        )

    def initialize(self) -> bool:
        """Inicializa o servi√ßo LLM."""
        try:
            if self.use_simulation:
                self.logger.info("üîÑ Inicializando LLM em modo simulado")
                self.is_initialized = True
                return True

            # Estrat√©gia de inicializa√ß√£o: AnythingLLM primeiro, depois fallbacks
            self.logger.info("üîÑ Inicializando servi√ßos LLM...")

            # 1. Tentar AnythingLLM (prioridade)
            if self.use_anythingllm and ANYTHINGLLM_AVAILABLE:
                try:
                    self.logger.info("üîÑ Tentando inicializar AnythingLLM...")
                    self.anythingllm_client = AnythingLLMClient(self.anythingllm_config)

                    if self.anythingllm_client.initialize():
                        self.is_initialized = True
                        self.logger.info("‚úÖ AnythingLLM inicializado com sucesso!")
                        return True
                    else:
                        self.logger.warning("‚ö†Ô∏è AnythingLLM falhou na inicializa√ß√£o")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erro no AnythingLLM: {e}")

            # 2. Tentar LLMWare (fallback)
            if LLMWARE_AVAILABLE:
                try:
                    self.logger.info("üîÑ Tentando inicializar LLMWare...")
                    self.llmware_service = LLMWareService()

                    if self.llmware_service.initialize():
                        self.is_initialized = True
                        self.logger.info("‚úÖ LLMWare Service inicializado com sucesso!")
                        return True
                    else:
                        self.logger.warning("‚ö†Ô∏è LLMWare falhou na inicializa√ß√£o")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erro no LLMWare: {e}")

            # 3. √öltimo recurso: simula√ß√£o
            self.logger.warning("‚ö†Ô∏è Nenhum servi√ßo LLM conseguiu inicializar, usando simula√ß√£o")
            self.use_simulation = True
            self.is_initialized = True
            return True

        except Exception as e:
            self.logger.error(f"‚ùå Erro geral na inicializa√ß√£o: {e}")
            # Fallback para simula√ß√£o em caso de erro
            self.use_simulation = True
            self.is_initialized = True
            return True

    def generate_response(self, prompt: str, max_tokens: int = 256, include_history: bool = True) -> str:
        """
        Gera resposta usando o modelo.

        Args:
            prompt: Texto de entrada
            max_tokens: N√∫mero m√°ximo de tokens para gerar
            include_history: Se deve incluir hist√≥rico da conversa

        Returns:
            Resposta gerada pelo modelo
        """
        if not self.is_initialized:
            return "‚ùå Servi√ßo n√£o inicializado"

        try:
            # Construir prompt com hist√≥rico se solicitado
            full_prompt = self._build_prompt_with_history(prompt) if include_history else prompt

            # Estrat√©gia de gera√ß√£o: AnythingLLM primeiro
            if self.anythingllm_client and not self.use_simulation:
                try:
                    self.logger.debug("ü§ñ Gerando resposta com AnythingLLM...")
                    response_obj = self.anythingllm_client.generate_response(
                        prompt=full_prompt,
                        max_tokens=max_tokens
                    )
                    response = response_obj.text

                    # Log de performance
                    self.logger.debug(f"üìä AnythingLLM: {response_obj.processing_time_ms:.1f}ms, "
                                    f"confian√ßa: {response_obj.confidence:.2f}")

                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erro no AnythingLLM, tentando fallback: {e}")
                    response = None

            # Fallback para LLMWare
            if response is None and self.llmware_service and not self.use_simulation:
                try:
                    self.logger.debug("ü§ñ Gerando resposta com LLMWare...")
                    response = self.llmware_service.generate_response(full_prompt, max_tokens)
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erro no LLMWare: {e}")
                    response = None

            # √öltimo fallback: simula√ß√£o
            if response is None or self.use_simulation:
                self.logger.debug("ü§ñ Gerando resposta simulada...")
                response = self._generate_simulation_response(prompt)

            # Adicionar ao hist√≥rico
            if include_history and response:
                self.conversation_history.append({"user": prompt, "assistant": response})
                # Limitar hist√≥rico para n√£o crescer indefinidamente
                if len(self.conversation_history) > 10:
                    self.conversation_history = self.conversation_history[-10:]

            return response

        except Exception as e:
            self.logger.error(f"‚ùå Erro na gera√ß√£o: {e}")
            return f"‚ùå Erro: {str(e)}"

    def _build_prompt_with_history(self, prompt: str) -> str:
        """Constr√≥i prompt incluindo hist√≥rico da conversa."""
        if not self.conversation_history:
            return prompt
        
        history_text = ""
        for entry in self.conversation_history[-5:]:  # √öltimas 5 intera√ß√µes
            history_text += f"Usu√°rio: {entry['user']}\nAssistente: {entry['assistant']}\n\n"
        
        return f"Hist√≥rico da conversa:\n{history_text}Nova pergunta: {prompt}"

    def _generate_simulation_response(self, prompt: str) -> str:
        """Gera resposta simulada para desenvolvimento/teste."""
        import random
        
        responses = [
            f"üí° Baseado em '{prompt[:30]}...', sugiro abordar o ROI direto.",
            f"üéØ Para a situa√ß√£o mencionada, recomendo focar nos benef√≠cios de longo prazo.",
            f"üìä Analisando o contexto, a melhor estrat√©gia seria evidenciar o valor agregado.",
            f"üîç Considerando a obje√ß√£o, sugiro apresentar cases de sucesso similares.",
            f"‚ö° Para superar essa barreira, foque na urg√™ncia da solu√ß√£o."
        ]
        
        base_response = random.choice(responses)
        return f"{base_response}\n\nü§ñ (Resposta simulada - modelo real n√£o carregado)"

    def clear_history(self):
        """Limpa o hist√≥rico da conversa."""
        self.conversation_history = []
        self.logger.info("üßπ Hist√≥rico da conversa limpo")

    def cleanup(self):
        """Limpa recursos do servi√ßo."""
        try:
            # Limpar AnythingLLM
            if self.anythingllm_client and hasattr(self.anythingllm_client, 'cleanup'):
                self.anythingllm_client.cleanup()

            # Limpar LLMWare
            if self.llmware_service and hasattr(self.llmware_service, 'cleanup'):
                self.llmware_service.cleanup()

            self.clear_history()
            self.is_initialized = False
            self.logger.info("üßπ LLMService limpo")

        except Exception as e:
            self.logger.error(f"‚ùå Erro na limpeza: {e}")

    def get_status(self) -> Dict[str, Any]:
        """Retorna status do servi√ßo."""
        status = {
            "is_initialized": self.is_initialized,
            "use_simulation": self.use_simulation,
            "anythingllm_available": ANYTHINGLLM_AVAILABLE,
            "llmware_available": LLMWARE_AVAILABLE,
            "conversation_length": len(self.conversation_history)
        }

        # Informa√ß√µes espec√≠ficas dos servi√ßos
        if self.anythingllm_client:
            status["anythingllm_stats"] = self.anythingllm_client.get_stats()

        return status


