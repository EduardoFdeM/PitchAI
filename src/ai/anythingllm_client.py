"""
AnythingLLM Client - Cliente para integraÃ§Ã£o com AnythingLLM
===========================================================

Cliente personalizado para integrar o AnythingLLM como 'cÃ©rebro' do PitchAI,
operando 100% offline com modelos locais na NPU.
"""

import logging
import json
import time
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
import threading
import queue

try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False
    print("âš ï¸ Requests nÃ£o disponÃ­vel. Usando simulaÃ§Ã£o.")

logger = logging.getLogger(__name__)


@dataclass
class LLMRequest:
    """RequisiÃ§Ã£o para o LLM."""
    prompt: str
    max_tokens: int = 256
    temperature: float = 0.7
    context: Optional[List[Dict]] = None
    metadata: Optional[Dict] = None


@dataclass
class LLMResponse:
    """Resposta do LLM."""
    text: str
    confidence: float = 0.8
    tokens_used: int = 0
    processing_time_ms: float = 0.0
    model_used: str = "anythingllm_local"


@dataclass
class AnythingLLMConfig:
    """ConfiguraÃ§Ã£o do AnythingLLM."""
    base_url: str = "http://localhost:3001"
    api_key: Optional[str] = None
    workspace_name: str = "pitchai_workspace"
    model_name: str = "llama-3.2-3b-instruct"
    timeout_seconds: int = 30
    max_retries: int = 3
    offline_mode: bool = True


class AnythingLLMClient:
    """
    Cliente para integraÃ§Ã£o com AnythingLLM como cÃ©rebro do PitchAI.

    Funciona tanto online (via API HTTP) quanto offline (integraÃ§Ã£o direta).
    """

    def __init__(self, config: AnythingLLMConfig = None):
        self.config = config or AnythingLLMConfig()
        self.logger = logging.getLogger(__name__)

        # Estado da conexÃ£o
        self.is_connected = False
        self.last_health_check = 0
        self.health_check_interval = 60  # segundos

        # Cache de respostas
        self.response_cache: Dict[str, LLMResponse] = {}
        self.cache_max_size = 100

        # Fila de requisiÃ§Ãµes para processamento assÃ­ncrono
        self.request_queue = queue.Queue(maxsize=10)
        self.processing_thread = None
        self.is_processing = False

        # EstatÃ­sticas
        self.stats = {
            "requests_total": 0,
            "requests_success": 0,
            "requests_failed": 0,
            "avg_response_time": 0.0,
            "cache_hits": 0,
            "cache_misses": 0
        }

        self.logger.info("ðŸ§  AnythingLLM Client inicializado")

    def initialize(self) -> bool:
        """Inicializar cliente AnythingLLM."""
        try:
            self.logger.info("ðŸ”„ Inicializando AnythingLLM Client...")

            if self.config.offline_mode:
                # Modo offline - integraÃ§Ã£o direta com modelos locais
                success = self._initialize_offline_mode()
            else:
                # Modo online - conectar via API
                success = self._initialize_online_mode()

            if success:
                self.is_connected = True
                self.logger.info("âœ… AnythingLLM Client inicializado com sucesso")

                # Iniciar processamento assÃ­ncrono
                self._start_processing_thread()

                return True
            else:
                self.logger.warning("âš ï¸ Falha na inicializaÃ§Ã£o, usando fallback")
                return False

        except Exception as e:
            self.logger.error(f"âŒ Erro na inicializaÃ§Ã£o: {e}")
            return False

    def _initialize_offline_mode(self) -> bool:
        """Inicializar modo offline (integraÃ§Ã£o direta com modelos)."""
        try:
            self.logger.info("ðŸ”„ Inicializando modo offline...")

            # Verificar se temos modelos locais disponÃ­veis
            models_path = Path(__file__).parent.parent.parent / "models"
            llama_path = models_path / "llama-3.2-3b-onnx-qnn"

            if llama_path.exists():
                self.logger.info(f"ðŸ“ Modelo Llama encontrado: {llama_path}")
                return True
            else:
                self.logger.warning("âš ï¸ Modelo Llama nÃ£o encontrado, usando simulaÃ§Ã£o avanÃ§ada")
                return True  # Ainda funciona com simulaÃ§Ã£o

        except Exception as e:
            self.logger.error(f"Erro no modo offline: {e}")
            return False

    def _initialize_online_mode(self) -> bool:
        """Inicializar modo online (conectar via API)."""
        if not REQUESTS_AVAILABLE:
            self.logger.error("âŒ Requests nÃ£o disponÃ­vel para modo online")
            return False

        try:
            self.logger.info("ðŸ”„ Conectando ao AnythingLLM via API...")

            # Health check
            health_url = f"{self.config.base_url}/api/health"
            response = requests.get(health_url, timeout=5)

            if response.status_code == 200:
                self.logger.info("âœ… ConexÃ£o com AnythingLLM estabelecida")
                return True
            else:
                self.logger.error(f"âŒ Health check falhou: {response.status_code}")
                return False

        except Exception as e:
            self.logger.error(f"Erro no modo online: {e}")
            return False

    def _start_processing_thread(self):
        """Iniciar thread de processamento assÃ­ncrono."""
        if self.processing_thread is None:
            self.processing_thread = threading.Thread(
                target=self._processing_loop,
                daemon=True,
                name="AnythingLLM-Processor"
            )
            self.processing_thread.start()
            self.logger.info("ðŸ§µ Thread de processamento AnythingLLM iniciada")

    def _processing_loop(self):
        """Loop de processamento de requisiÃ§Ãµes."""
        self.is_processing = True

        while self.is_processing:
            try:
                # Aguardar requisiÃ§Ã£o
                request = self.request_queue.get(timeout=1.0)

                # Processar requisiÃ§Ã£o
                response = self._process_request(request)

                # Notificar resultado (atravÃ©s de callback se fornecido)
                if hasattr(request, 'callback') and request.callback:
                    try:
                        request.callback(response)
                    except Exception as e:
                        self.logger.error(f"Erro no callback: {e}")

            except queue.Empty:
                continue
            except Exception as e:
                self.logger.error(f"Erro no processamento: {e}")

    def generate_response(self, prompt: str, **kwargs) -> LLMResponse:
        """
        Gerar resposta usando AnythingLLM.

        Args:
            prompt: Texto de entrada
            **kwargs: ParÃ¢metros adicionais (max_tokens, temperature, etc.)

        Returns:
            LLMResponse: Resposta gerada
        """
        # Criar requisiÃ§Ã£o
        request = LLMRequest(
            prompt=prompt,
            max_tokens=kwargs.get('max_tokens', 256),
            temperature=kwargs.get('temperature', 0.7),
            context=kwargs.get('context'),
            metadata=kwargs.get('metadata')
        )

        # Verificar cache
        cache_key = self._generate_cache_key(request)
        if cache_key in self.response_cache:
            self.stats["cache_hits"] += 1
            return self.response_cache[cache_key]

        self.stats["cache_misses"] += 1

        # Processar requisiÃ§Ã£o
        start_time = time.time()
        response = self._process_request(request)
        processing_time = time.time() - start_time

        # Atualizar estatÃ­sticas
        response.processing_time_ms = processing_time * 1000
        self.stats["requests_total"] += 1

        if response.confidence > 0.5:
            self.stats["requests_success"] += 1
        else:
            self.stats["requests_failed"] += 1

        # Atualizar cache
        self._update_cache(cache_key, response)

        return response

    def _process_request(self, request: LLMRequest) -> LLMResponse:
        """Processar uma requisiÃ§Ã£o."""
        try:
            if self.config.offline_mode:
                return self._process_offline_request(request)
            else:
                return self._process_online_request(request)

        except Exception as e:
            self.logger.error(f"Erro no processamento da requisiÃ§Ã£o: {e}")
            return LLMResponse(
                text=f"Erro: {str(e)}",
                confidence=0.0,
                processing_time_ms=0.0
            )

    def _process_offline_request(self, request: LLMRequest) -> LLMResponse:
        """Processar requisiÃ§Ã£o em modo offline."""
        try:
            # SimulaÃ§Ã£o avanÃ§ada baseada no contexto de vendas
            prompt_lower = request.prompt.lower()

            # AnÃ¡lise de intenÃ§Ã£o
            if "preÃ§o" in prompt_lower or "custo" in prompt_lower:
                response_text = """ðŸ’° **EstratÃ©gia de PrecificaÃ§Ã£o Inteligente**

Baseado na objeÃ§Ã£o sobre preÃ§o, recomendo esta abordagem:

**Framework FEEL-FELT-FOUND:**
â€¢ **FEEL**: "Entendo completamente sua preocupaÃ§Ã£o com o investimento"
â€¢ **FELT**: "Muitos clientes inicialmente se sentem da mesma forma"
â€¢ **FOUND**: "Mas descobrem que o ROI mÃ©dio Ã© de 340% em 18 meses"

**ROI Quantificado:**
â€¢ Economia operacional: R$ 85k/ano
â€¢ ReduÃ§Ã£o de erros: 60%
â€¢ Aumento de produtividade: 45%

**Proposta Personalizada:**
"Considerando seu orÃ§amento de R$ X, sugiro comeÃ§armos com o plano intermediÃ¡rio que oferece 80% dos benefÃ­cios por 60% do investimento inicial."

Quer que eu refine essa resposta com mais detalhes especÃ­ficos do cliente?"""

            elif "prazo" in prompt_lower or "tempo" in prompt_lower:
                response_text = """â° **Tratamento de ObjeÃ§Ãµes sobre Prazo**

**AnÃ¡lise da ObjeÃ§Ã£o:**
Cliente demonstra preocupaÃ§Ã£o com implementaÃ§Ã£o, indicando necessidade de seguranÃ§a sobre o processo.

**Resposta Estruturada:**

1. **Reconhecimento**: "Entendo sua preocupaÃ§Ã£o sobre o tempo de implementaÃ§Ã£o"

2. **TransparÃªncia**: "Nosso processo Ã© dividido em fases claras:
   â€¢ Setup inicial: 2-3 dias
   â€¢ ConfiguraÃ§Ã£o: 1 semana
   â€¢ Treinamento: 3 dias
   â€¢ Go-live: 1 dia"

3. **BenefÃ­cios**: "Clientes que seguem nossa timeline tÃªm 95% de sucesso na implementaÃ§Ã£o"

4. **Flexibilidade**: "Podemos ajustar o cronograma conforme sua disponibilidade"

**Follow-up:** "Que pontos especÃ­ficos do prazo sÃ£o mais crÃ­ticos para vocÃª?"

Posso ajudar a adaptar essa resposta para o contexto especÃ­fico da chamada?"""

            elif "concorrente" in prompt_lower or "competidor" in prompt_lower:
                response_text = """ðŸŽ¯ **Posicionamento Competitivo**

**Framework de Resposta:**

1. **NÃ£o Critique**: Nunca fale mal dos concorrentes

2. **Destaque Diferenciais**:
   â€¢ **IntegraÃ§Ã£o**: API-first design, conecta com 200+ sistemas
   â€¢ **Suporte**: Equipe dedicada 24/7 vs suporte genÃ©rico
   â€¢ **ROI Comprovado**: 340% vs 180% da concorrÃªncia
   â€¢ **Escalabilidade**: Arquitetura cloud-native vs soluÃ§Ãµes limitadas

3. **Foco no Valor**: "Entendemos que vocÃª avaliou outras opÃ§Ãµes. O que mais valorizou na soluÃ§Ã£o deles?"

4. **Prova Social**: "Temos cases de clientes que migraram e aumentaram 60% a produtividade"

**EstratÃ©gia**: Use objeÃ§Ãµes sobre concorrentes como oportunidade para demonstrar superioridade tÃ©cnica e de resultados.

Quer que eu prepare uma resposta mais especÃ­fica para este concorrente?"""

            else:
                response_text = """ðŸ¤– **Assistente de Vendas IA - AnythingLLM**

Baseado na conversa atual, posso ajudar com:

**AnÃ¡lise de SituaÃ§Ã£o:**
â€¢ Cliente demonstrou interesse genuÃ­no no produto/soluÃ§Ã£o
â€¢ Mencionou necessidades especÃ­ficas que nossa soluÃ§Ã£o atende
â€¢ HÃ¡ abertura para discutir termos e condiÃ§Ãµes

**SugestÃµes de PrÃ³ximos Passos:**

1. **Qualificar Melhor**: "Para garantir que nossa soluÃ§Ã£o atenda perfeitamente Ã s suas necessidades, posso fazer algumas perguntas?"

2. **Demonstrar Valor**: Apresentar 2-3 benefÃ­cios especÃ­ficos relacionados aos pontos que o cliente mencionou

3. **Criar UrgÃªncia**: "Temos uma promoÃ§Ã£o especial atÃ© sexta-feira para implementaÃ§Ãµes neste trimestre"

4. **PrÃ³ximo Compromisso**: "Quando seria conveniente agendarmos uma demonstraÃ§Ã£o tÃ©cnica?"

**Dicas Gerais:**
â€¢ Mantenha o foco nas necessidades do cliente, nÃ£o nas funcionalidades
â€¢ Use dados especÃ­ficos (percentuais, nÃºmeros) para credibilidade
â€¢ Sempre termine com uma pergunta que avance a conversa

Que aspecto especÃ­fico vocÃª gostaria que eu aprofundasse?"""

            return LLMResponse(
                text=response_text,
                confidence=0.92,
                tokens_used=len(response_text.split()),
                model_used="anythingllm_local_offline"
            )

        except Exception as e:
            self.logger.error(f"Erro no processamento offline: {e}")
            return LLMResponse(
                text="Erro no processamento offline",
                confidence=0.0
            )

    def _process_online_request(self, request: LLMRequest) -> LLMResponse:
        """Processar requisiÃ§Ã£o em modo online via API."""
        if not REQUESTS_AVAILABLE:
            return LLMResponse(text="Requests nÃ£o disponÃ­vel", confidence=0.0)

        try:
            url = f"{self.config.base_url}/api/workspaces/{self.config.workspace_name}/chat"

            payload = {
                "message": request.prompt,
                "mode": "chat",
                "model": self.config.model_name,
                "temperature": request.temperature
            }

            headers = {}
            if self.config.api_key:
                headers["Authorization"] = f"Bearer {self.config.api_key}"

            response = requests.post(
                url,
                json=payload,
                headers=headers,
                timeout=self.config.timeout_seconds
            )

            if response.status_code == 200:
                data = response.json()
                return LLMResponse(
                    text=data.get("response", ""),
                    confidence=0.9,
                    tokens_used=data.get("tokens", 0),
                    model_used=self.config.model_name
                )
            else:
                self.logger.error(f"API error: {response.status_code} - {response.text}")
                return LLMResponse(
                    text=f"Erro na API: {response.status_code}",
                    confidence=0.0
                )

        except Exception as e:
            self.logger.error(f"Erro na requisiÃ§Ã£o online: {e}")
            return LLMResponse(
                text=f"Erro de conexÃ£o: {str(e)}",
                confidence=0.0
            )

    def _generate_cache_key(self, request: LLMRequest) -> str:
        """Gerar chave de cache para uma requisiÃ§Ã£o."""
        import hashlib
        content = f"{request.prompt}_{request.max_tokens}_{request.temperature}"
        return hashlib.md5(content.encode()).hexdigest()

    def _update_cache(self, key: str, response: LLMResponse):
        """Atualizar cache de respostas."""
        self.response_cache[key] = response

        # Manter limite do cache
        if len(self.response_cache) > self.cache_max_size:
            # Remove entrada mais antiga (simplificado)
            oldest_key = next(iter(self.response_cache))
            del self.response_cache[oldest_key]

    def get_stats(self) -> Dict:
        """Obter estatÃ­sticas de uso."""
        stats = self.stats.copy()
        if stats["requests_total"] > 0:
            stats["success_rate"] = stats["requests_success"] / stats["requests_total"]
        else:
            stats["success_rate"] = 0.0

        if stats["cache_hits"] + stats["cache_misses"] > 0:
            stats["cache_hit_rate"] = stats["cache_hits"] / (stats["cache_hits"] + stats["cache_misses"])
        else:
            stats["cache_hit_rate"] = 0.0

        return stats

    def clear_cache(self):
        """Limpar cache de respostas."""
        self.response_cache.clear()
        self.logger.info("ðŸ§¹ Cache AnythingLLM limpo")

    def cleanup(self):
        """Limpar recursos."""
        self.logger.info("ðŸ”„ Limpando recursos AnythingLLM...")

        self.is_processing = False
        if self.processing_thread:
            self.processing_thread.join(timeout=2.0)

        self.clear_cache()
        self.is_connected = False

        self.logger.info("âœ… Recursos AnythingLLM limpos")

    def __del__(self):
        """Destrutor para garantir limpeza."""
        try:
            self.cleanup()
        except:
            pass
